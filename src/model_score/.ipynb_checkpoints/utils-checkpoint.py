import os
import pickle
import yaml
import functools
import gc 
import torch

with open("paths.yaml", "r") as f:
    paths = yaml.safe_load(f)

CACHE = paths["CACHE"]
    
def cache(file_name_func):
    os.makedirs(CACHE, exist_ok=True)
    os.makedirs(os.path.join(CACHE, 'r_scores'), exist_ok=True)
    os.makedirs(os.path.join(CACHE, 'tokens'), exist_ok=True)
        
    def decorator(func):
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            file_name = file_name_func(*args, **kwargs)
            cache_path = os.path.join(CACHE, file_name)

            if os.path.exists(cache_path):
                print(f'scores for {cache_path} already exist')
                return

            result = func(self, *args, **kwargs)
            with open(cache_path, 'wb') as file:
                pickle.dump(result, file)
            gc.collect()

        return wrapper
    return decorator


import torch
import numpy as np

def match_tokens_and_average_embeddings(original_tokens, original_embeddings, reference_tokens):
    """
    Optimized alignment of original tokenization with the reference tokenization.
    """
    aligned_tokens = []  # To store the final tokens that match reference tokens
    aligned_embeddings = []  # To store the final embeddings

    # Pre-strip space prefixes from original tokens (e.g., 'Ġ')
    stripped_tokens = [token.replace('Ġ', '') for token in original_tokens]

    original_index = 0
    for ref_token in reference_tokens:
        current_word = []
        current_embeddings = []

        while "".join(current_word) != ref_token:
            current_word.append(stripped_tokens[original_index])
            current_embeddings.append(original_embeddings[original_index])
            original_index += 1

        # Efficient averaging using torch.mean
        aligned_embeddings.append(torch.mean(torch.stack(current_embeddings), dim=0))
        aligned_tokens.append(ref_token)

    # Convert aligned_embeddings to a single tensor
    aligned_embeddings = torch.stack(aligned_embeddings)

    return aligned_embeddings, aligned_tokens



# def match_tokens_and_average_embeddings(original_tokens, original_embeddings, reference_tokens):
#     """
#     Aligns original tokenization with the reference tokenization by merging subword tokens 
#     in the original tokens that correspond to a single word in the reference tokens. 
#     Averages the embeddings for merged tokens.

#     Args:
#     - original_tokens: A list of tokens (strings) generated by a tokenizer (the original tokens).
#     - original_embeddings: A tensor of shape (num_tokens, embedding_dim) containing embeddings for each token.
#     - reference_tokens: A list of reference tokens (strings) that should be the final desired tokens.

#     Returns:
#     - aligned_embeddings: A tensor of shape (num_reference_tokens, embedding_dim), where embeddings for tokens 
#       that are part of the same word in the reference tokens are averaged.
#     - aligned_tokens: A list of reference tokens (strings) that match the new tokens.
#     """
    
#     aligned_tokens = []  # To store the final tokens that match reference tokens
#     aligned_embeddings = []  # To store the final embeddings
    
#     current_word = ""  # To accumulate subword tokens from the original token list
#     current_embeddings = []  # To accumulate embeddings for the same word
#     original_index = 0  # Index to iterate through original tokens
    
#     for ref_token in reference_tokens:
#         current_word = ""
#         current_embeddings = []

#         while current_word != ref_token:
#             original_token = original_tokens[original_index].replace('Ġ', '')  # Clean space token prefix (GPT-2 specific)
#             current_word += original_token
#             # print(ref_token, current_word)
#             current_embeddings.append(original_embeddings[original_index])
#             original_index += 1
    
#             # If the current accumulated word matches the reference token, break and move on
#             if current_word == ref_token:
#                 break

#         avg_embedding = torch.mean(torch.stack(current_embeddings), dim=0)
#         aligned_embeddings.append(avg_embedding)
#         aligned_tokens.append(ref_token)
    
#     # Convert aligned_embeddings list into a tensor
#     if len(aligned_embeddings) > 0:
#         aligned_embeddings = torch.stack(aligned_embeddings)
#     else:
#         aligned_embeddings = torch.tensor([])  # Empty tensor in case of no alignments

#     return aligned_embeddings, aligned_tokens
